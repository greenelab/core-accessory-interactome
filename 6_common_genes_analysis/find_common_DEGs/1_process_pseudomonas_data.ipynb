{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process pseudomonas data\n",
    "This notebook trains a VAE on the PAO1 and PA14 RNA-seq compendia\n",
    "\n",
    "1. Selects template experiment from the compendium\n",
    "2. Normalizes the gene expression data from the RNA-seq Pseudomonas compendium\n",
    "3. Train VAE on the normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alexandra/anaconda3/envs/generic_expression_new/lib/python3.7/site-packages/ponyo/helper_vae.py:21: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alexandra/anaconda3/envs/generic_expression_new/lib/python3.7/site-packages/ponyo/helper_vae.py:25: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alexandra/anaconda3/envs/generic_expression_new/lib/python3.7/site-packages/ponyo/helper_vae.py:25: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/alexandra/anaconda3/envs/generic_expression_new/lib/python3.7/site-packages/matplotlib/__init__.py:886: MatplotlibDeprecationWarning: \n",
      "examples.directory is deprecated; in the future, examples will be found relative to the 'datapath' directory.\n",
      "  \"found relative to the 'datapath' directory.\".format(key))\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "\n",
    "from ponyo import utils, train_vae_modules\n",
    "from generic_expression_patterns_modules import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alexandra/Documents/Repos/core-accessory-interactome/6_common_genes_analysis/find_common_DEGs/generic_expression_patterns_modules/process.py:57: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set seeds to get reproducible VAE trained models\n",
    "process.set_all_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters for data processing\n",
    "\n",
    "Most parameters are read from `config_filename`. We manually selected bioproject [GEOD-33245](https://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-33245/?s_sortby=col_8&s_sortorder=ascending), as the template experiment, which contains multiple different comparisons including WT vs *crc* mutants, WT vs *cbr* mutants in different conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "\n",
    "# Read in config variables\n",
    "config_filename = \"config_pseudomonas_pa14_rnaseq.tsv\"\n",
    "\n",
    "params = utils.read_config(config_filename)\n",
    "\n",
    "local_dir = params[\"local_dir\"]\n",
    "dataset_name = params[\"dataset_name\"]\n",
    "\n",
    "# Column header containing sample ids\n",
    "metadata_colname = params[\"metadata_colname\"]\n",
    "\n",
    "# Template experiment ID\n",
    "project_id = params[\"project_id\"]\n",
    "\n",
    "# Output file: pickled list of shared genes(generated during gene ID mapping)\n",
    "shared_genes_filename = params[\"shared_genes_filename\"]\n",
    "\n",
    "# Output files of pseudomonas template experiment data\n",
    "raw_template_filename = params[\"raw_template_filename\"]\n",
    "processed_template_filename = params[\"processed_template_filename\"]\n",
    "\n",
    "# Output files of pseudomonas compendium data\n",
    "# raw_compendium_filename = params['raw_compendium_filename']\n",
    "processed_compendium_filename = params[\"processed_compendium_filename\"]\n",
    "normalized_compendium_filename = params[\"normalized_compendium_filename\"]\n",
    "\n",
    "# Output file: pickled scaler (generated during compendium normalization)\n",
    "scaler_filename = params[\"scaler_filename\"]\n",
    "\n",
    "# Load metadata file with mapping between experiments and associated samples\n",
    "metadata_filename = \"data/metadata/SraRunTable.csv\"\n",
    "metadata_delimiter = \",\"\n",
    "experiment_id_colname = \"SRA_study\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize compendium\n",
    "The compendium is MR normalized, but here we will 0-1 normalize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: dataset contains 576 samples and 5887 genes\n"
     ]
    }
   ],
   "source": [
    "process.normalize_compendium(\n",
    "    processed_compendium_filename,\n",
    "    normalized_compendium_filename,\n",
    "    scaler_filename,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get raw pseudomonas template experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.process_raw_template_pseudomonas(\n",
    "    processed_compendium_filename,\n",
    "    project_id,\n",
    "    metadata_filename,\n",
    "    metadata_delimiter,\n",
    "    experiment_id_colname,\n",
    "    metadata_colname,\n",
    "    raw_template_filename,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = pd.read_csv(raw_template_filename, sep=\"\\t\", index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PA14_55610</th>\n",
       "      <th>PA14_55600</th>\n",
       "      <th>PA14_55590</th>\n",
       "      <th>PA14_55580</th>\n",
       "      <th>PA14_55570</th>\n",
       "      <th>PA14_55560</th>\n",
       "      <th>PA14_55550</th>\n",
       "      <th>PA14_55540</th>\n",
       "      <th>PA14_55530</th>\n",
       "      <th>PA14_55520</th>\n",
       "      <th>...</th>\n",
       "      <th>PA14_19205</th>\n",
       "      <th>PA14_17675</th>\n",
       "      <th>PA14_67975</th>\n",
       "      <th>PA14_36345</th>\n",
       "      <th>PA14_43405</th>\n",
       "      <th>PA14_38825</th>\n",
       "      <th>PA14_24245</th>\n",
       "      <th>PA14_28895</th>\n",
       "      <th>PA14_55117</th>\n",
       "      <th>PA14_59845</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SRX1740204</th>\n",
       "      <td>179.103190</td>\n",
       "      <td>76.046134</td>\n",
       "      <td>33.867541</td>\n",
       "      <td>1142.769777</td>\n",
       "      <td>356.544170</td>\n",
       "      <td>104.303714</td>\n",
       "      <td>31.166448</td>\n",
       "      <td>85.188292</td>\n",
       "      <td>75.838358</td>\n",
       "      <td>5.402184</td>\n",
       "      <td>...</td>\n",
       "      <td>48.619660</td>\n",
       "      <td>19.738751</td>\n",
       "      <td>261.382614</td>\n",
       "      <td>275.719181</td>\n",
       "      <td>0.623329</td>\n",
       "      <td>56.515160</td>\n",
       "      <td>135.054610</td>\n",
       "      <td>156.871124</td>\n",
       "      <td>63.164002</td>\n",
       "      <td>261.382614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRX1740205</th>\n",
       "      <td>160.820796</td>\n",
       "      <td>65.859945</td>\n",
       "      <td>31.972706</td>\n",
       "      <td>1395.886215</td>\n",
       "      <td>435.939228</td>\n",
       "      <td>115.829263</td>\n",
       "      <td>37.333399</td>\n",
       "      <td>101.470264</td>\n",
       "      <td>58.201812</td>\n",
       "      <td>4.786333</td>\n",
       "      <td>...</td>\n",
       "      <td>45.757345</td>\n",
       "      <td>25.654746</td>\n",
       "      <td>389.798976</td>\n",
       "      <td>251.952580</td>\n",
       "      <td>0.957267</td>\n",
       "      <td>116.212170</td>\n",
       "      <td>125.593383</td>\n",
       "      <td>164.075502</td>\n",
       "      <td>58.776172</td>\n",
       "      <td>296.561205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRX1740206</th>\n",
       "      <td>156.853998</td>\n",
       "      <td>69.081551</td>\n",
       "      <td>34.540776</td>\n",
       "      <td>695.749907</td>\n",
       "      <td>207.394180</td>\n",
       "      <td>56.820323</td>\n",
       "      <td>20.634749</td>\n",
       "      <td>52.334508</td>\n",
       "      <td>55.325052</td>\n",
       "      <td>4.037234</td>\n",
       "      <td>...</td>\n",
       "      <td>38.877063</td>\n",
       "      <td>23.326238</td>\n",
       "      <td>253.897129</td>\n",
       "      <td>341.220995</td>\n",
       "      <td>1.644799</td>\n",
       "      <td>126.350456</td>\n",
       "      <td>133.826814</td>\n",
       "      <td>136.368776</td>\n",
       "      <td>56.521269</td>\n",
       "      <td>306.829746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRX1740207</th>\n",
       "      <td>230.669314</td>\n",
       "      <td>54.576966</td>\n",
       "      <td>20.524671</td>\n",
       "      <td>474.399781</td>\n",
       "      <td>174.692938</td>\n",
       "      <td>39.183463</td>\n",
       "      <td>15.393503</td>\n",
       "      <td>47.346684</td>\n",
       "      <td>74.635167</td>\n",
       "      <td>5.364403</td>\n",
       "      <td>...</td>\n",
       "      <td>20.058201</td>\n",
       "      <td>24.722899</td>\n",
       "      <td>173.060294</td>\n",
       "      <td>483.029473</td>\n",
       "      <td>0.466470</td>\n",
       "      <td>113.352160</td>\n",
       "      <td>197.083488</td>\n",
       "      <td>102.156885</td>\n",
       "      <td>72.069583</td>\n",
       "      <td>390.901688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRX1740208</th>\n",
       "      <td>199.661033</td>\n",
       "      <td>64.058640</td>\n",
       "      <td>32.725610</td>\n",
       "      <td>562.776041</td>\n",
       "      <td>169.372437</td>\n",
       "      <td>59.010541</td>\n",
       "      <td>19.147963</td>\n",
       "      <td>52.743935</td>\n",
       "      <td>86.165834</td>\n",
       "      <td>8.529547</td>\n",
       "      <td>...</td>\n",
       "      <td>27.677510</td>\n",
       "      <td>21.584977</td>\n",
       "      <td>165.890989</td>\n",
       "      <td>368.337180</td>\n",
       "      <td>1.218507</td>\n",
       "      <td>95.043526</td>\n",
       "      <td>143.435650</td>\n",
       "      <td>125.506194</td>\n",
       "      <td>71.717825</td>\n",
       "      <td>330.911616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5887 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            PA14_55610  PA14_55600  PA14_55590   PA14_55580  PA14_55570  \\\n",
       "SRX1740204  179.103190   76.046134   33.867541  1142.769777  356.544170   \n",
       "SRX1740205  160.820796   65.859945   31.972706  1395.886215  435.939228   \n",
       "SRX1740206  156.853998   69.081551   34.540776   695.749907  207.394180   \n",
       "SRX1740207  230.669314   54.576966   20.524671   474.399781  174.692938   \n",
       "SRX1740208  199.661033   64.058640   32.725610   562.776041  169.372437   \n",
       "\n",
       "            PA14_55560  PA14_55550  PA14_55540  PA14_55530  PA14_55520  \\\n",
       "SRX1740204  104.303714   31.166448   85.188292   75.838358    5.402184   \n",
       "SRX1740205  115.829263   37.333399  101.470264   58.201812    4.786333   \n",
       "SRX1740206   56.820323   20.634749   52.334508   55.325052    4.037234   \n",
       "SRX1740207   39.183463   15.393503   47.346684   74.635167    5.364403   \n",
       "SRX1740208   59.010541   19.147963   52.743935   86.165834    8.529547   \n",
       "\n",
       "               ...      PA14_19205  PA14_17675  PA14_67975  PA14_36345  \\\n",
       "SRX1740204     ...       48.619660   19.738751  261.382614  275.719181   \n",
       "SRX1740205     ...       45.757345   25.654746  389.798976  251.952580   \n",
       "SRX1740206     ...       38.877063   23.326238  253.897129  341.220995   \n",
       "SRX1740207     ...       20.058201   24.722899  173.060294  483.029473   \n",
       "SRX1740208     ...       27.677510   21.584977  165.890989  368.337180   \n",
       "\n",
       "            PA14_43405  PA14_38825  PA14_24245  PA14_28895  PA14_55117  \\\n",
       "SRX1740204    0.623329   56.515160  135.054610  156.871124   63.164002   \n",
       "SRX1740205    0.957267  116.212170  125.593383  164.075502   58.776172   \n",
       "SRX1740206    1.644799  126.350456  133.826814  136.368776   56.521269   \n",
       "SRX1740207    0.466470  113.352160  197.083488  102.156885   72.069583   \n",
       "SRX1740208    1.218507   95.043526  143.435650  125.506194   71.717825   \n",
       "\n",
       "            PA14_59845  \n",
       "SRX1740204  261.382614  \n",
       "SRX1740205  296.561205  \n",
       "SRX1740206  306.829746  \n",
       "SRX1740207  390.901688  \n",
       "SRX1740208  330.911616  \n",
       "\n",
       "[5 rows x 5887 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 5887)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "* We are training our VAE model using ALL the data in the compendium.\n",
    "* The template experiment is using a subset of the samples in the real experiment and using those in the DE analysis in order to ensure the comparison of samples with consistent backgrounds (i.e. some experiments have samples with 3 different biological conditions and for now our statistical test is doing a binary comparison).\n",
    "* Simulated experiments are generated by shifting the template experiment (using ALL samples in the real experiment) in the latent space. Then dropping the samples to match the template experiment and perform DE analysis.\n",
    "\n",
    "\n",
    "So there is an inconsistency in the samples used to learn a low-dimensional representation and those used to calculate DE statistics. This inconsistency should not not change the simulated experiments since all samples in the template experiment are moved the same amount in the latent space. The only way for this inconsistency to effect the simulated experiments is if the low dimensional space is significantly different including all the experiment samples vs only including a subset. However, we believe that such few samples will likely not effect the space. Furthermore, the dataset used to train the VAE should be a general representation of gene expression patterns and shouldn't have to be include the template experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VAE directories if needed\n",
    "output_dirs = [\n",
    "    os.path.join(base_dir, dataset_name, \"models\"),\n",
    "    os.path.join(base_dir, dataset_name, \"logs\"),\n",
    "]\n",
    "\n",
    "NN_architecture = params[\"NN_architecture\"]\n",
    "\n",
    "# Check if NN architecture directory exist otherwise create\n",
    "for each_dir in output_dirs:\n",
    "    sub_dir = os.path.join(each_dir, NN_architecture)\n",
    "    os.makedirs(sub_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dataset contains 576 samples and 5887 genes\n",
      "WARNING:tensorflow:From /home/alexandra/anaconda3/envs/generic_expression_new/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "tracking <tf.Variable 'Variable:0' shape=() dtype=float32> beta\n",
      "WARNING:tensorflow:From /home/alexandra/anaconda3/envs/generic_expression_new/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandra/anaconda3/envs/generic_expression_new/lib/python3.7/site-packages/keras/engine/training_utils.py:819: UserWarning: Output custom_variational_layer_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_1.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alexandra/anaconda3/envs/generic_expression_new/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 432 samples, validate on 144 samples\n",
      "Epoch 1/120\n",
      "432/432 [==============================] - 27s 63ms/step - loss: 2283.0599 - kl_loss: 14.4142 - recons_loss: 2283.0598 - val_loss: 2072.2133 - val_kl_loss: 11.3726 - val_recons_loss: 2072.2131\n",
      "Epoch 2/120\n",
      "432/432 [==============================] - 22s 50ms/step - loss: 2023.2903 - kl_loss: 14.0545 - recons_loss: 2023.2904 - val_loss: 2023.5950 - val_kl_loss: 12.1395 - val_recons_loss: 2023.5951\n",
      "Epoch 3/120\n",
      "432/432 [==============================] - 22s 50ms/step - loss: 2006.5272 - kl_loss: 14.4879 - recons_loss: 2006.5271 - val_loss: 2021.7129 - val_kl_loss: 13.7628 - val_recons_loss: 2021.7129\n",
      "Epoch 4/120\n",
      "432/432 [==============================] - 22s 50ms/step - loss: 1992.3631 - kl_loss: 14.8924 - recons_loss: 1992.3628 - val_loss: 1981.2521 - val_kl_loss: 14.1386 - val_recons_loss: 1981.2522\n",
      "Epoch 5/120\n",
      "432/432 [==============================] - 23s 53ms/step - loss: 1979.5231 - kl_loss: 15.2980 - recons_loss: 1979.5228 - val_loss: 1961.4110 - val_kl_loss: 14.4277 - val_recons_loss: 1961.4110\n",
      "Epoch 6/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1978.1578 - kl_loss: 15.5743 - recons_loss: 1978.1580 - val_loss: 1947.0870 - val_kl_loss: 13.3038 - val_recons_loss: 1947.0870\n",
      "Epoch 7/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1968.4209 - kl_loss: 15.9116 - recons_loss: 1968.4205 - val_loss: 1959.1949 - val_kl_loss: 14.5042 - val_recons_loss: 1959.1948\n",
      "Epoch 8/120\n",
      "432/432 [==============================] - 23s 53ms/step - loss: 1964.6208 - kl_loss: 16.4668 - recons_loss: 1964.6208 - val_loss: 1950.5767 - val_kl_loss: 14.4180 - val_recons_loss: 1950.5767\n",
      "Epoch 9/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1964.5922 - kl_loss: 16.6755 - recons_loss: 1964.5922 - val_loss: 1939.2923 - val_kl_loss: 14.0117 - val_recons_loss: 1939.2921\n",
      "Epoch 10/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1961.8038 - kl_loss: 17.3688 - recons_loss: 1961.8036 - val_loss: 1940.8915 - val_kl_loss: 16.7301 - val_recons_loss: 1940.8915\n",
      "Epoch 11/120\n",
      "432/432 [==============================] - 23s 53ms/step - loss: 1954.4589 - kl_loss: 18.1011 - recons_loss: 1954.4586 - val_loss: 1927.8840 - val_kl_loss: 16.8768 - val_recons_loss: 1927.8839\n",
      "Epoch 12/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1941.0611 - kl_loss: 18.6921 - recons_loss: 1941.0612 - val_loss: 1924.0095 - val_kl_loss: 17.2347 - val_recons_loss: 1924.0095\n",
      "Epoch 13/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1935.7008 - kl_loss: 19.3202 - recons_loss: 1935.7008 - val_loss: 1917.0887 - val_kl_loss: 17.2923 - val_recons_loss: 1917.0885\n",
      "Epoch 14/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1931.3356 - kl_loss: 19.6241 - recons_loss: 1931.3357 - val_loss: 1920.5345 - val_kl_loss: 17.0869 - val_recons_loss: 1920.5343\n",
      "Epoch 15/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1935.9708 - kl_loss: 20.2479 - recons_loss: 1935.9709 - val_loss: 1916.4227 - val_kl_loss: 17.9741 - val_recons_loss: 1916.4227\n",
      "Epoch 16/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1932.9791 - kl_loss: 21.0431 - recons_loss: 1932.9789 - val_loss: 1910.0244 - val_kl_loss: 20.0251 - val_recons_loss: 1910.0245\n",
      "Epoch 17/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1920.4612 - kl_loss: 21.6980 - recons_loss: 1920.4611 - val_loss: 1905.6328 - val_kl_loss: 20.8321 - val_recons_loss: 1905.6326\n",
      "Epoch 18/120\n",
      "432/432 [==============================] - 23s 54ms/step - loss: 1918.6603 - kl_loss: 22.4989 - recons_loss: 1918.6600 - val_loss: 1906.5109 - val_kl_loss: 20.6698 - val_recons_loss: 1906.5109\n",
      "Epoch 19/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1917.1928 - kl_loss: 23.0868 - recons_loss: 1917.1927 - val_loss: 1902.0945 - val_kl_loss: 20.5367 - val_recons_loss: 1902.0946\n",
      "Epoch 20/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1910.1497 - kl_loss: 23.7481 - recons_loss: 1910.1498 - val_loss: 1901.0734 - val_kl_loss: 21.8000 - val_recons_loss: 1901.0734\n",
      "Epoch 21/120\n",
      "432/432 [==============================] - 23s 52ms/step - loss: 1905.4462 - kl_loss: 24.5755 - recons_loss: 1905.4463 - val_loss: 1898.0674 - val_kl_loss: 21.6956 - val_recons_loss: 1898.0673\n",
      "Epoch 22/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1899.1199 - kl_loss: 25.2489 - recons_loss: 1899.1200 - val_loss: 1889.8819 - val_kl_loss: 21.6652 - val_recons_loss: 1889.8820\n",
      "Epoch 23/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1903.4259 - kl_loss: 25.9783 - recons_loss: 1903.4260 - val_loss: 1889.1437 - val_kl_loss: 23.3552 - val_recons_loss: 1889.1437\n",
      "Epoch 24/120\n",
      "432/432 [==============================] - 23s 52ms/step - loss: 1895.3013 - kl_loss: 26.6360 - recons_loss: 1895.3016 - val_loss: 1884.3035 - val_kl_loss: 23.2633 - val_recons_loss: 1884.3033\n",
      "Epoch 25/120\n",
      "432/432 [==============================] - 23s 52ms/step - loss: 1890.4058 - kl_loss: 27.1924 - recons_loss: 1890.4054 - val_loss: 1881.4832 - val_kl_loss: 23.7900 - val_recons_loss: 1881.4833\n",
      "Epoch 26/120\n",
      "432/432 [==============================] - 23s 54ms/step - loss: 1882.1730 - kl_loss: 28.0080 - recons_loss: 1882.1727 - val_loss: 1882.4496 - val_kl_loss: 23.8906 - val_recons_loss: 1882.4497\n",
      "Epoch 27/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1880.7293 - kl_loss: 28.8295 - recons_loss: 1880.7290 - val_loss: 1875.0136 - val_kl_loss: 24.9361 - val_recons_loss: 1875.0137\n",
      "Epoch 28/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1879.7259 - kl_loss: 29.5032 - recons_loss: 1879.7257 - val_loss: 1871.3648 - val_kl_loss: 25.7956 - val_recons_loss: 1871.3646\n",
      "Epoch 29/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1873.8591 - kl_loss: 30.1903 - recons_loss: 1873.8593 - val_loss: 1879.9441 - val_kl_loss: 26.2805 - val_recons_loss: 1879.9440\n",
      "Epoch 30/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1875.7636 - kl_loss: 30.9032 - recons_loss: 1875.7633 - val_loss: 1872.9526 - val_kl_loss: 25.4332 - val_recons_loss: 1872.9525\n",
      "Epoch 31/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1873.8332 - kl_loss: 31.4190 - recons_loss: 1873.8335 - val_loss: 1871.3047 - val_kl_loss: 27.3826 - val_recons_loss: 1871.3049\n",
      "Epoch 32/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1867.3437 - kl_loss: 32.3154 - recons_loss: 1867.3433 - val_loss: 1869.1438 - val_kl_loss: 29.1307 - val_recons_loss: 1869.1439\n",
      "Epoch 33/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1864.4661 - kl_loss: 33.1491 - recons_loss: 1864.4662 - val_loss: 1859.7353 - val_kl_loss: 29.4173 - val_recons_loss: 1859.7352\n",
      "Epoch 34/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1857.4547 - kl_loss: 33.9554 - recons_loss: 1857.4547 - val_loss: 1865.9029 - val_kl_loss: 28.8552 - val_recons_loss: 1865.9028\n",
      "Epoch 35/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1858.0190 - kl_loss: 34.5205 - recons_loss: 1858.0188 - val_loss: 1862.1495 - val_kl_loss: 30.6096 - val_recons_loss: 1862.1495\n",
      "Epoch 36/120\n",
      "432/432 [==============================] - 25s 58ms/step - loss: 1856.7043 - kl_loss: 35.3599 - recons_loss: 1856.7041 - val_loss: 1858.6151 - val_kl_loss: 30.7576 - val_recons_loss: 1858.6150\n",
      "Epoch 37/120\n",
      "432/432 [==============================] - 23s 54ms/step - loss: 1857.2825 - kl_loss: 35.8797 - recons_loss: 1857.2823 - val_loss: 1865.7315 - val_kl_loss: 30.9207 - val_recons_loss: 1865.7313\n",
      "Epoch 38/120\n",
      "432/432 [==============================] - 23s 53ms/step - loss: 1851.2926 - kl_loss: 36.7818 - recons_loss: 1851.2924 - val_loss: 1859.6387 - val_kl_loss: 31.3747 - val_recons_loss: 1859.6387\n",
      "Epoch 39/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432/432 [==============================] - 22s 51ms/step - loss: 1852.6875 - kl_loss: 37.7153 - recons_loss: 1852.6875 - val_loss: 1859.3757 - val_kl_loss: 31.8551 - val_recons_loss: 1859.3756\n",
      "Epoch 40/120\n",
      "432/432 [==============================] - 23s 52ms/step - loss: 1851.0299 - kl_loss: 38.3632 - recons_loss: 1851.0299 - val_loss: 1850.9495 - val_kl_loss: 32.7907 - val_recons_loss: 1850.9497\n",
      "Epoch 41/120\n",
      "432/432 [==============================] - 23s 54ms/step - loss: 1847.6412 - kl_loss: 39.2296 - recons_loss: 1847.6412 - val_loss: 1858.2139 - val_kl_loss: 35.1827 - val_recons_loss: 1858.2140\n",
      "Epoch 42/120\n",
      "432/432 [==============================] - 23s 53ms/step - loss: 1843.7412 - kl_loss: 40.1660 - recons_loss: 1843.7413 - val_loss: 1850.4691 - val_kl_loss: 35.6316 - val_recons_loss: 1850.4690\n",
      "Epoch 43/120\n",
      "432/432 [==============================] - 23s 54ms/step - loss: 1843.4530 - kl_loss: 41.0516 - recons_loss: 1843.4530 - val_loss: 1852.9183 - val_kl_loss: 36.3757 - val_recons_loss: 1852.9182\n",
      "Epoch 44/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1842.6986 - kl_loss: 41.5025 - recons_loss: 1842.6990 - val_loss: 1851.1125 - val_kl_loss: 36.7450 - val_recons_loss: 1851.1124\n",
      "Epoch 45/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1838.8966 - kl_loss: 42.5059 - recons_loss: 1838.8962 - val_loss: 1854.5231 - val_kl_loss: 35.8592 - val_recons_loss: 1854.5229\n",
      "Epoch 46/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1835.5630 - kl_loss: 43.3580 - recons_loss: 1835.5632 - val_loss: 1843.7885 - val_kl_loss: 38.0118 - val_recons_loss: 1843.7885\n",
      "Epoch 47/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1837.9555 - kl_loss: 44.0604 - recons_loss: 1837.9557 - val_loss: 1848.9020 - val_kl_loss: 37.5425 - val_recons_loss: 1848.9019\n",
      "Epoch 48/120\n",
      "432/432 [==============================] - 23s 54ms/step - loss: 1836.9606 - kl_loss: 44.5202 - recons_loss: 1836.9604 - val_loss: 1849.1409 - val_kl_loss: 37.1230 - val_recons_loss: 1849.1411\n",
      "Epoch 49/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1835.1939 - kl_loss: 45.3405 - recons_loss: 1835.1940 - val_loss: 1843.9183 - val_kl_loss: 39.1498 - val_recons_loss: 1843.9182\n",
      "Epoch 50/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1830.7318 - kl_loss: 46.3346 - recons_loss: 1830.7313 - val_loss: 1845.2729 - val_kl_loss: 38.5270 - val_recons_loss: 1845.2729\n",
      "Epoch 51/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1833.8883 - kl_loss: 47.1652 - recons_loss: 1833.8885 - val_loss: 1848.7173 - val_kl_loss: 39.6468 - val_recons_loss: 1848.7174\n",
      "Epoch 52/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1829.2965 - kl_loss: 47.8866 - recons_loss: 1829.2966 - val_loss: 1840.8731 - val_kl_loss: 40.7092 - val_recons_loss: 1840.8733\n",
      "Epoch 53/120\n",
      "432/432 [==============================] - 23s 53ms/step - loss: 1827.1531 - kl_loss: 48.5141 - recons_loss: 1827.1531 - val_loss: 1843.7614 - val_kl_loss: 41.7787 - val_recons_loss: 1843.7612\n",
      "Epoch 54/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1825.5971 - kl_loss: 49.1977 - recons_loss: 1825.5968 - val_loss: 1842.0366 - val_kl_loss: 44.3040 - val_recons_loss: 1842.0365\n",
      "Epoch 55/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1827.9250 - kl_loss: 50.1475 - recons_loss: 1827.9248 - val_loss: 1841.5028 - val_kl_loss: 43.8798 - val_recons_loss: 1841.5028\n",
      "Epoch 56/120\n",
      "432/432 [==============================] - 23s 53ms/step - loss: 1826.0009 - kl_loss: 50.8818 - recons_loss: 1826.0007 - val_loss: 1837.0481 - val_kl_loss: 43.9881 - val_recons_loss: 1837.0482\n",
      "Epoch 57/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1823.1970 - kl_loss: 51.7103 - recons_loss: 1823.1970 - val_loss: 1838.4505 - val_kl_loss: 43.2533 - val_recons_loss: 1838.4506\n",
      "Epoch 58/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1823.5999 - kl_loss: 52.1492 - recons_loss: 1823.6000 - val_loss: 1838.6860 - val_kl_loss: 44.3763 - val_recons_loss: 1838.6862\n",
      "Epoch 59/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1818.7258 - kl_loss: 53.0928 - recons_loss: 1818.7258 - val_loss: 1835.9983 - val_kl_loss: 46.1024 - val_recons_loss: 1835.9980\n",
      "Epoch 60/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1816.4615 - kl_loss: 54.0273 - recons_loss: 1816.4614 - val_loss: 1834.7119 - val_kl_loss: 46.7907 - val_recons_loss: 1834.7118\n",
      "Epoch 61/120\n",
      "432/432 [==============================] - 23s 53ms/step - loss: 1818.6991 - kl_loss: 54.8255 - recons_loss: 1818.6990 - val_loss: 1838.8404 - val_kl_loss: 48.7664 - val_recons_loss: 1838.8403\n",
      "Epoch 62/120\n",
      "432/432 [==============================] - 23s 53ms/step - loss: 1813.5172 - kl_loss: 55.5361 - recons_loss: 1813.5172 - val_loss: 1831.3172 - val_kl_loss: 46.6219 - val_recons_loss: 1831.3173\n",
      "Epoch 63/120\n",
      "432/432 [==============================] - 23s 54ms/step - loss: 1812.5907 - kl_loss: 56.2918 - recons_loss: 1812.5909 - val_loss: 1834.8126 - val_kl_loss: 46.4791 - val_recons_loss: 1834.8125\n",
      "Epoch 64/120\n",
      "432/432 [==============================] - 23s 52ms/step - loss: 1814.9084 - kl_loss: 57.1802 - recons_loss: 1814.9083 - val_loss: 1833.6236 - val_kl_loss: 49.8307 - val_recons_loss: 1833.6235\n",
      "Epoch 65/120\n",
      "432/432 [==============================] - 23s 54ms/step - loss: 1814.8640 - kl_loss: 57.9491 - recons_loss: 1814.8641 - val_loss: 1832.8648 - val_kl_loss: 51.8177 - val_recons_loss: 1832.8650\n",
      "Epoch 66/120\n",
      "432/432 [==============================] - 23s 53ms/step - loss: 1815.8760 - kl_loss: 58.6405 - recons_loss: 1815.8759 - val_loss: 1835.2796 - val_kl_loss: 50.5663 - val_recons_loss: 1835.2795\n",
      "Epoch 67/120\n",
      "432/432 [==============================] - 24s 55ms/step - loss: 1813.7834 - kl_loss: 59.1948 - recons_loss: 1813.7837 - val_loss: 1831.6825 - val_kl_loss: 51.4844 - val_recons_loss: 1831.6825\n",
      "Epoch 68/120\n",
      "432/432 [==============================] - 23s 54ms/step - loss: 1811.7021 - kl_loss: 60.1886 - recons_loss: 1811.7024 - val_loss: 1830.7976 - val_kl_loss: 52.8906 - val_recons_loss: 1830.7975\n",
      "Epoch 69/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1807.6459 - kl_loss: 60.9159 - recons_loss: 1807.6459 - val_loss: 1828.6608 - val_kl_loss: 53.3921 - val_recons_loss: 1828.6608\n",
      "Epoch 70/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1811.5457 - kl_loss: 61.6223 - recons_loss: 1811.5453 - val_loss: 1830.0810 - val_kl_loss: 52.1875 - val_recons_loss: 1830.0809\n",
      "Epoch 71/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1805.5744 - kl_loss: 62.4887 - recons_loss: 1805.5745 - val_loss: 1830.9251 - val_kl_loss: 53.3747 - val_recons_loss: 1830.9249\n",
      "Epoch 72/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1808.4726 - kl_loss: 62.9423 - recons_loss: 1808.4727 - val_loss: 1833.0468 - val_kl_loss: 56.4325 - val_recons_loss: 1833.0469\n",
      "Epoch 73/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1808.5347 - kl_loss: 63.6866 - recons_loss: 1808.5349 - val_loss: 1828.4278 - val_kl_loss: 55.7866 - val_recons_loss: 1828.4277\n",
      "Epoch 74/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1806.4225 - kl_loss: 64.5875 - recons_loss: 1806.4229 - val_loss: 1824.9358 - val_kl_loss: 55.4802 - val_recons_loss: 1824.9360\n",
      "Epoch 75/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1801.4021 - kl_loss: 65.3105 - recons_loss: 1801.4022 - val_loss: 1826.7109 - val_kl_loss: 55.7130 - val_recons_loss: 1826.7107\n",
      "Epoch 76/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1801.5120 - kl_loss: 65.9994 - recons_loss: 1801.5120 - val_loss: 1828.2476 - val_kl_loss: 55.2640 - val_recons_loss: 1828.2476\n",
      "Epoch 77/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1802.9735 - kl_loss: 66.5924 - recons_loss: 1802.9734 - val_loss: 1828.1200 - val_kl_loss: 57.0426 - val_recons_loss: 1828.1200\n",
      "Epoch 78/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1802.5694 - kl_loss: 67.2942 - recons_loss: 1802.5693 - val_loss: 1825.1562 - val_kl_loss: 57.6184 - val_recons_loss: 1825.1562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1799.2623 - kl_loss: 68.1498 - recons_loss: 1799.2623 - val_loss: 1823.7103 - val_kl_loss: 57.4227 - val_recons_loss: 1823.7103\n",
      "Epoch 80/120\n",
      "432/432 [==============================] - 23s 53ms/step - loss: 1801.4118 - kl_loss: 69.0023 - recons_loss: 1801.4117 - val_loss: 1830.0215 - val_kl_loss: 60.0030 - val_recons_loss: 1830.0215\n",
      "Epoch 81/120\n",
      "432/432 [==============================] - 23s 54ms/step - loss: 1802.4660 - kl_loss: 69.6878 - recons_loss: 1802.4659 - val_loss: 1822.0183 - val_kl_loss: 61.7065 - val_recons_loss: 1822.0184\n",
      "Epoch 82/120\n",
      "432/432 [==============================] - 23s 54ms/step - loss: 1797.2242 - kl_loss: 70.5025 - recons_loss: 1797.2240 - val_loss: 1821.7182 - val_kl_loss: 63.6101 - val_recons_loss: 1821.7183\n",
      "Epoch 83/120\n",
      "432/432 [==============================] - 24s 54ms/step - loss: 1798.1611 - kl_loss: 71.2429 - recons_loss: 1798.1611 - val_loss: 1821.3030 - val_kl_loss: 62.6684 - val_recons_loss: 1821.3030\n",
      "Epoch 84/120\n",
      "432/432 [==============================] - 23s 54ms/step - loss: 1800.9455 - kl_loss: 72.0062 - recons_loss: 1800.9456 - val_loss: 1824.0712 - val_kl_loss: 63.1239 - val_recons_loss: 1824.0712\n",
      "Epoch 85/120\n",
      "432/432 [==============================] - 23s 54ms/step - loss: 1798.7198 - kl_loss: 73.0013 - recons_loss: 1798.7197 - val_loss: 1823.4024 - val_kl_loss: 63.1699 - val_recons_loss: 1823.4023\n",
      "Epoch 86/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1795.3098 - kl_loss: 73.8774 - recons_loss: 1795.3097 - val_loss: 1824.2008 - val_kl_loss: 64.0569 - val_recons_loss: 1824.2009\n",
      "Epoch 87/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1796.5128 - kl_loss: 74.3587 - recons_loss: 1796.5126 - val_loss: 1821.1151 - val_kl_loss: 62.9352 - val_recons_loss: 1821.1150\n",
      "Epoch 88/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1794.3662 - kl_loss: 75.0580 - recons_loss: 1794.3663 - val_loss: 1820.2757 - val_kl_loss: 62.8294 - val_recons_loss: 1820.2756\n",
      "Epoch 89/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1793.9669 - kl_loss: 75.7339 - recons_loss: 1793.9667 - val_loss: 1817.6289 - val_kl_loss: 64.4804 - val_recons_loss: 1817.6285\n",
      "Epoch 90/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1794.5186 - kl_loss: 76.4655 - recons_loss: 1794.5186 - val_loss: 1820.2196 - val_kl_loss: 67.3083 - val_recons_loss: 1820.2197\n",
      "Epoch 91/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1793.0696 - kl_loss: 77.1310 - recons_loss: 1793.0695 - val_loss: 1819.7264 - val_kl_loss: 69.2700 - val_recons_loss: 1819.7264\n",
      "Epoch 92/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1792.3043 - kl_loss: 78.0084 - recons_loss: 1792.3040 - val_loss: 1819.0354 - val_kl_loss: 66.9824 - val_recons_loss: 1819.0355\n",
      "Epoch 93/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1789.2977 - kl_loss: 78.9171 - recons_loss: 1789.2980 - val_loss: 1815.8901 - val_kl_loss: 67.1096 - val_recons_loss: 1815.8903\n",
      "Epoch 94/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1792.8118 - kl_loss: 79.5545 - recons_loss: 1792.8120 - val_loss: 1813.8785 - val_kl_loss: 67.4835 - val_recons_loss: 1813.8784\n",
      "Epoch 95/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1790.3501 - kl_loss: 80.5004 - recons_loss: 1790.3499 - val_loss: 1819.9659 - val_kl_loss: 68.1120 - val_recons_loss: 1819.9658\n",
      "Epoch 96/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1789.6607 - kl_loss: 81.0979 - recons_loss: 1789.6608 - val_loss: 1813.4386 - val_kl_loss: 66.4674 - val_recons_loss: 1813.4387\n",
      "Epoch 97/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1789.4318 - kl_loss: 81.7473 - recons_loss: 1789.4321 - val_loss: 1817.9022 - val_kl_loss: 67.8251 - val_recons_loss: 1817.9022\n",
      "Epoch 98/120\n",
      "432/432 [==============================] - 22s 52ms/step - loss: 1788.6451 - kl_loss: 82.7331 - recons_loss: 1788.6451 - val_loss: 1812.5221 - val_kl_loss: 69.2018 - val_recons_loss: 1812.5220\n",
      "Epoch 99/120\n",
      "432/432 [==============================] - 23s 53ms/step - loss: 1788.4050 - kl_loss: 83.4618 - recons_loss: 1788.4049 - val_loss: 1816.4859 - val_kl_loss: 71.8285 - val_recons_loss: 1816.4860\n",
      "Epoch 100/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1786.3085 - kl_loss: 84.2197 - recons_loss: 1786.3085 - val_loss: 1816.6425 - val_kl_loss: 74.0726 - val_recons_loss: 1816.6426\n",
      "Epoch 101/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1787.8546 - kl_loss: 84.7786 - recons_loss: 1787.8544 - val_loss: 1815.2388 - val_kl_loss: 77.4174 - val_recons_loss: 1815.2388\n",
      "Epoch 102/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1784.3273 - kl_loss: 85.9224 - recons_loss: 1784.3273 - val_loss: 1813.8912 - val_kl_loss: 79.3861 - val_recons_loss: 1813.8911\n",
      "Epoch 103/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1788.1798 - kl_loss: 86.3348 - recons_loss: 1788.1799 - val_loss: 1821.4723 - val_kl_loss: 76.6273 - val_recons_loss: 1821.4722\n",
      "Epoch 104/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1785.7678 - kl_loss: 87.4970 - recons_loss: 1785.7679 - val_loss: 1814.7889 - val_kl_loss: 79.3582 - val_recons_loss: 1814.7889\n",
      "Epoch 105/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1785.6782 - kl_loss: 88.2102 - recons_loss: 1785.6781 - val_loss: 1817.4738 - val_kl_loss: 75.6108 - val_recons_loss: 1817.4739\n",
      "Epoch 106/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1783.4889 - kl_loss: 88.9352 - recons_loss: 1783.4885 - val_loss: 1812.4156 - val_kl_loss: 78.1433 - val_recons_loss: 1812.4156\n",
      "Epoch 107/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1778.4677 - kl_loss: 89.7574 - recons_loss: 1778.4674 - val_loss: 1812.6177 - val_kl_loss: 78.3401 - val_recons_loss: 1812.6177\n",
      "Epoch 108/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1782.2347 - kl_loss: 90.3412 - recons_loss: 1782.2349 - val_loss: 1812.6958 - val_kl_loss: 80.5392 - val_recons_loss: 1812.6958\n",
      "Epoch 109/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1783.7935 - kl_loss: 90.8740 - recons_loss: 1783.7935 - val_loss: 1810.4447 - val_kl_loss: 83.7970 - val_recons_loss: 1810.4447\n",
      "Epoch 110/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1781.1315 - kl_loss: 91.9111 - recons_loss: 1781.1315 - val_loss: 1810.8664 - val_kl_loss: 80.1859 - val_recons_loss: 1810.8662\n",
      "Epoch 111/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1785.0301 - kl_loss: 92.4949 - recons_loss: 1785.0302 - val_loss: 1816.0565 - val_kl_loss: 81.1882 - val_recons_loss: 1816.0566\n",
      "Epoch 112/120\n",
      "432/432 [==============================] - 23s 52ms/step - loss: 1781.8352 - kl_loss: 93.1796 - recons_loss: 1781.8351 - val_loss: 1815.2166 - val_kl_loss: 80.3767 - val_recons_loss: 1815.2166\n",
      "Epoch 113/120\n",
      "432/432 [==============================] - 23s 54ms/step - loss: 1781.6171 - kl_loss: 93.8592 - recons_loss: 1781.6172 - val_loss: 1813.1592 - val_kl_loss: 84.7066 - val_recons_loss: 1813.1592\n",
      "Epoch 114/120\n",
      "432/432 [==============================] - 23s 53ms/step - loss: 1782.5061 - kl_loss: 94.4645 - recons_loss: 1782.5061 - val_loss: 1810.4853 - val_kl_loss: 85.7187 - val_recons_loss: 1810.4855\n",
      "Epoch 115/120\n",
      "432/432 [==============================] - 23s 52ms/step - loss: 1778.5504 - kl_loss: 95.2088 - recons_loss: 1778.5505 - val_loss: 1813.4000 - val_kl_loss: 84.1634 - val_recons_loss: 1813.4000\n",
      "Epoch 116/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1779.0799 - kl_loss: 96.1720 - recons_loss: 1779.0798 - val_loss: 1810.8628 - val_kl_loss: 84.9541 - val_recons_loss: 1810.8628\n",
      "Epoch 117/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1779.5369 - kl_loss: 96.9507 - recons_loss: 1779.5370 - val_loss: 1810.7386 - val_kl_loss: 85.2825 - val_recons_loss: 1810.7388\n",
      "Epoch 118/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1780.4327 - kl_loss: 97.9122 - recons_loss: 1780.4326 - val_loss: 1810.8779 - val_kl_loss: 90.9500 - val_recons_loss: 1810.8781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1777.0869 - kl_loss: 98.4098 - recons_loss: 1777.0867 - val_loss: 1810.1967 - val_kl_loss: 86.0743 - val_recons_loss: 1810.1968\n",
      "Epoch 120/120\n",
      "432/432 [==============================] - 22s 51ms/step - loss: 1777.8798 - kl_loss: 99.1303 - recons_loss: 1777.8799 - val_loss: 1813.2892 - val_kl_loss: 86.9456 - val_recons_loss: 1813.2893\n"
     ]
    }
   ],
   "source": [
    "# Train VAE on new compendium data\n",
    "train_vae_modules.train_vae(config_filename, normalized_compendium_filename)"
   ]
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1592246126078,
   "trusted": true
  },
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python [conda env:generic_expression_new] *",
   "language": "python",
   "name": "conda-env-generic_expression_new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
